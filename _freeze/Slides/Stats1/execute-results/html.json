{
  "hash": "959fa35bd4555fd885d8e6332363c4f7",
  "result": {
    "markdown": "---\ntitle: \"Stats 1 - General Linear Model\"\nsubtitle: |\n   | R Coding Club \n   | RTG 2660\nauthor: \"Dr. Lea Hildebrandt\"\ndate: 2024/04/16\ndescription: \"Brief Overview Basic Statistics\"\nformat: \n  revealjs:\n    smaller: true\n    scrollable: true\n    slide-number: true\n    theme: simple\n    embed-resources: true\neditor: visual\nfrom: markdown+emoji\n---\n\n\n# Basic Statistics\n\n\n::: {.cell}\n<style type=\"text/css\">\ncode.sourceCode {   font-size: 1.4em; }   \ndiv.cell-output-stdout {   font-size: 1.4em; }\n</style>\n:::\n\n\nNo warm-up today! It will be a bit more input today...\n\nToday (and next week), we will try to cover a lot!\n\n# Fitting Models to Data\n\n\n::: {.cell}\n\n:::\n\n\n::: notes\nno interactions planned, a lot of input\n\nbut feel free to ask questions anytime! It's important that you understand the concepts!\n\n(We might not manage to go through all the slides, but still ask questions - slides/video online)\n:::\n\n## What is a Model?\n\n> \" \"models\" are generally simplifications of things in the real world that nonetheless convey the essence of the thing being modeled\"\n>\n> \"All models are wrong but some are useful\" (G. Box)\n\n(ST21, Ch 5)\n\n**Aim**: Find the model that most efficiently and accurately summarizes the way in which the data were actually generated.\n\nBasic structure of statistical models:\n\n$$\ndata=model+error\n$$\n\n::: notes\na statistical model is generally much simpler than the data being described; it is meant to capture the structure of the data as simply as possible.\n\nTwo parts:\n\n-   one portion that is described by a statistical model, which expresses the values that we expect the data to take given our knowledge,\n\n-   *error* that reflects the difference between the model's predictions and the observed data.\n:::\n\n## Statistical Models\n\nIn general, we want to predict single observations (denoted by i) from the model. The fact that we are looking at predictions of observations and not actual values of the data is denoted by the \"hat\":\n\n$$\n\\widehat{data_i} = model_i\n$$ The error is then simply the deviation of the actual data from the predicted values:\n\n$$ \nerror_i = data_i - \\widehat{data_i}\n$$ If this doesn't make much sense yet, let's look at an example.\n\n::: notes\nThis means that the predicted value of the data for observation i is equal to the value of the model for that observation.\n:::\n\n## A Simple Model\n\nLet's say we want to have a model of height of children (in the NHANES dataset also used in ST21).\n\nWhat do you think would be a good model?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## A Simple Model 2\n\nThe simplest model would be... the mean of the height values! This would imply that the model would predict the same height for everyone - and all individual deviations would be part of the error term.\n\nWe can write such a simple model as a formula:\n\n$$\ny_i = \\beta + \\epsilon\n$$\n\n$y_i$ denotes the individual observations (hence the $i$) of heights, $\\beta$ is a so-called parameter, and $\\epsilon$ is the error term. In this example, the parameter $\\beta$ would be the same value (= the mean height) for everyone (hence it doesn't need a $i$ subscript). *Parameters* are values that we estimate to find the best model.\n\n## A Simple Model 3\n\nHow do we find parameters that belong to the best fitting model?\n\n. . .\n\nWe try to minimize the error!\n\nRemember, the error is the difference between the actual and predicted values of $y$ (height):\n\n$$\nerror_i = y_i - \\hat{y_i}\n$$\n\nIf we select a predicted value of 400cm, all individuals' errors would hugely deviate (because no one is 4m tall). If we average these errors, it would still be a big value.\n\nA better candidate for such a simple model is thus the arithmetic mean or average:\n\n$$\n\\bar{X} = \\frac{\\sum_{i=1}^{n}x_i}{n}\n$$\n\nSumming up all individual's heights and dividing that number by the number of individuals gives us the mean. By definition (see book for proof, the individual errors cancel out), the average error is now 0!\n\n## A Note on Errors\n\nWe usually don't simply average across the individual errors, but across the squared errors.\n\nThe reason is that positive and negative errors cancel each other out, which is not the case when squared.\n\nThe *mean squared error* would be in a different unit then the data (squared!), which is why we usually take the square root of that value to bring it back to the original unit: This leaves us with the *root mean squared error (RMSE)*!\n\n## A Slightly More Complex Model\n\nObviously, the model for predicting height from the average is not very good: It predicts the same height for all children! (The RMSE is 27 cm!)\n\nHow can we improve this model?\n\n. . .\n\nWe can account for other information that we might have!\\\nFor example, to account for age might be a good idea: Older children are likely taller than younger ones. We plot height against age to visually inspect the relationship:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n::: notes\nRMSE: On average, 27 cm \"wrong\" per individual!\n\nA: raw data, visible strong relationship\\\nB: only age (linear relationship)\\\nC: intercept/constant\\\nD: also account for gender\n\n--\\> line fits data increasingly better!\n:::\n\n## A Slightly More Complex Model 2\n\nAs we can see, the line (\\~ model) fits the data points increasingly well, e.g. if we include a constant (or intercept) and age. We would write this as this formula:\n\n$$\n\\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} * age_i\n$$\n\nRemember from linear algebra that this defines a line:\n\n$$\ny = slope * x + intercept\n$$\n\nThus $\\beta_0$ is the parameter for the intercept and $\\beta_1$ for the slope of age!\n\nThe model fit is now much better: RMSE = 8.36 cm.\n\n. . .\n\nAdding gender? Does not improve model too much!\n\n::: notes\nw/o intercept: A, no $\\beta_0$\n\nStats Software will estimate best values for $\\beta$'s\n:::\n\n## What is a \"Good\" Model?\n\nTwo aims:\n\n1.  Describe data well (= low error/RMSE)\n\n2.  Generalize to new data (low error when applied to new data)\n\nCan be conflicting!\n\n. . .\n\nWhere does error come from? (In addition to individual differences!)\n\n::: incremental\n-   measurement error (noise): random variation in data\n\n    -   actual measurement is biased (broken device, bias etc.)\n\n    -   \"thing measured\" may be biased/varies a lot\n\n-   wrong model specification\n\n    -   e.g. height goes *down* with age\n\n    -   important variable is missing from model (age!)\n:::\n\n## Examples Measurement Error\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Simulated relationship between blood alcohol content and reaction time on a driving test, with best-fitting linear model represented by the line. A: linear relationship with low measurement error.  B: linear relationship with higher measurement error.  C: Nonlinear relationship with low measurement error and (incorrect) linear model](Stats1_files/figure-revealjs/BACrt-1.png){width=960}\n:::\n:::\n\n\n::: notes\nA: very little error, all points close to fitted line\\\nB: same relationship much more variability across individuals\\\nC. wrongly specified model (caffeine!), not a linear relationship. Error high (deviations points - line)\n:::\n\n## Can a Model be too Good?\n\nYes! This is called overfitting.\n\n. . .\n\nIf we fit a line too closely to the data, the model might not be able to generalize to other data well.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![An example of overfitting. Both datasets were generated using the same model, with different random noise added to generate each set.  The left panel shows the data used to fit the model, with a simple linear fit in blue and a complex (8th order polynomial) fit in red.  The root mean square error (RMSE) values for each model are shown in the figure; in this case, the complex model has a lower RMSE than the simple model.  The right panel shows the second dataset, with the same model overlaid on it and the RMSE values computed using the model obtained from the first dataset.  Here we see that the simpler model actually fits the new dataset better than the more complex model, which was overfitted to the first dataset.](Stats1_files/figure-revealjs/Overfitting-1.png){width=768 height=50%}\n:::\n:::\n\n\n::: notes\nsame formula, different noise (simulation) \\~ different individuals\n\nsimpler model fits new data better!\n:::\n\n# Summarizing Data\n\n## Central Tendency\n\nWhy summarize data?\n\n. . .\n\nIt's a model & describes the data! E.g. the mean = central tendency of the data\n\n. . .\n\nMean, Median, Mode?\n\n. . .\n\n**Mean** = minimizes sum of squared error, but highly influenced by outliers!\\\n**Median** = \"middle\" value if ranked, minimizes sum of absolute error, less influenced by extreme values\\\n**Mode** = most often occurring value\n\n. . .\n\nExample:\n\nIf 3 people earn 10,000 Euros per *year* and 1 person earns 1,000,000:\\\nMean: 257,500 Euros\\\nMedian: (Rank: 10,000; 10,000; 10,000; 1,000,000 -\\> middle value = )10,000 Euros\\\nMode: 10,000 Euros\n\n::: notes\nexamples\n\nmean: income --\\> if one person earns a million and 3 only 10.000 --\\> mean = 257.500\n:::\n\n## Variability\n\nHow widespread are the data?\n\n. . .\n\n**Variance** and **Standard Deviation**\n\n**Variance** ≊ Mean Squared Error\n\n$$\n\\sigma^2 = \\frac{SSE}{N} = \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{N}\n$$\n\n(Note: $x_i$ = value of ind. observation, $\\mu$ = *population* mean instead of $\\hat{X}$ = *sample* mean)\n\n. . .\n\n**Standard Deviation** ≊ Root Mean Squared Error\n\n$$\nSD = \\sigma = \\sqrt{\\sigma^2}\n$$\n\n. . .\n\nWe usually don't know the population mean $\\mu$, thats why we estimate the sample variance (with the \"hat\"):\n\n$$\n\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n (x_i - \\hat{X})^2}{n-1}\n$$\n\nNote: we now use $\\hat{X}$ and $n$ for the *sample* size. $n-1$ is used to make the estimate more robust/less biased.\n\n::: notes\nRemember plot above: Points either close to line or wide-spread\n\nVariance = sigma\\^2, deviations of data points from mean ($\\mu$) squared and summed, divided by number of oberservations\n\n$n-1$ = Degrees of Freedom, one value is fixed if we know the mean.\n\nDifference MSE: MSE is diff to predicted value, not necessarily mean (based on formula w/intercept and slope, –\\> n-2)\n:::\n\n## Z-Scores\n\n$$\nZ(x) = \\frac{x - \\mu}{\\sigma}\n$$\n\n::: incremental\n-   standardizes the distribution: How far is any data point from the mean in units of SD?\n-   doesn't change original relationship of data points!\n    -   shifts distribution to have a mean = 0 and SD = 1.\n-   useful if we compare (or use in a model) variables on different scales/units!\n:::\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Density (top) and cumulative distribution (bottom) of a standard normal distribution, with cutoffs at one standard deviation above/below the mean.](Stats1_files/figure-revealjs/zDensityCDF-1.png){width=480}\n:::\n:::\n\n\n::: notes\nZ of x\n\nx is single value/data point\\\nmu, sigma\n\nz-scores directly comparable\n:::\n\n# Probability, Sampling, Null-Hypothesis Testing...\n\nLet's skip this (for now?) - but you should know that there are different probability distributions (normal, uniform etc.) and that we draw samples from the population when we run experiments...\\\n(Also, some classical probability theory and resampling methods like Monte Carlo simulations are helpful to know).\n\nWe also skip Null Hypothesis Significance Testing, which is the main approach we're using in psychology etc., as well as Confidence Intervals, Effect Sizes... You should have a rough idea of these concepts.\n\n# The General Linear Model\n\n\n::: {.cell}\n\n:::\n\n\nRemember the basic model of statistics:\n\n$$ data = model + error $$\n\nOur general goal is to find the model with the *best fit*, i.e. that minimizes the error.\n\n. . .\n\nOne approach is the GLM. You might be surprised that a lot of the common models can be viewed as linear models:\n\n::: scrollable\n![All models can be thought of as linear models](images/linear_tests_cheat_sheet.png)\n:::\n\n## Definitions\n\n**Dependent variable (DV)**: The outcome variable that the model aims to explain ($Y$).\n\n**Independent variable (IV)**: The variable that we use to explain the DV ($X$).\n\n**Linear model**: The model for the DV is composed of a *linear combination* of IVs (that are multiplied by different [weights]{.underline}!)\n\n. . .\n\nThe weights are the *parameters* $\\beta$ and determine the relative contribution of each IV. (This is what the model estimates! The weights thus give us the important information we're usually interested in: How strong are IV and DV related.)\n\nThere may be several DVs, but usually that's not the case and we will focus on those cases with one DV!\n\n## Example\n\n::: columns\n::: column\nLet's use some simulated data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-6-1.png){width=288 height=50%}\n:::\n:::\n\n:::\n\n::: column\nWe can calculate the *correlation* between the two variables:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  df$grade and df$studyTime\nt = 2, df = 6, p-value = 0.09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.13  0.93\nsample estimates:\n cor \n0.63 \n```\n:::\n:::\n\n\nThe correlation is quite high (.63), but the CI is also pretty wide.\n:::\n:::\n\n. . .\n\nFundamental activities of statistics:\n\n-   *Describe*: How strong is the relationship between grade and study time?\n\n-   *Decide*: Is there a statistically significant relationship between grade and study time?\n\n-   *Predict*: Given a particular amount of study time, what grade do we expect?\n\n## Linear Regression\n\nUse the GLM (\\~synonymous to linear regression) to...\n\n::: incremental\n-   decribe the relation between two variables (similar to correlation)\n\n-   predict DV for new values of IV (new observations)\n\n-   add multiple IVs!\n:::\n\n. . .\n\n::: columns\n::: column\nSimple GLM:\n\n$$ y = \\beta_0+ x * \\beta_x + \\epsilon $$\n\n$\\beta_0$ = *intercept*, the overall offset of the line when $x=0$ (even if that is impossible)\\\n$\\beta_x$ = *slope*, how much do we expect $y$ to change with each change in $x$?\\\n$y$ = *DV*\\\n$x$ = *IV* or *predictor\\\n*$\\epsilon$ = *error term*, whatever variance is left over once the model is fit, *residuals*! (Think of the model as the line that is fitted and the residuals are the vertical deviations of the data points from the line!)\n\n(If we refer to *predicted* $y$-values, after we have estimated the model fit/line, we can drop the error term: $\\hat{y} = \\hat{\\beta_0} + x * \\hat{\\beta_x}$.)\n:::\n\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## The Relation Between Correlation and Regression\n\nThere is a close relation and we can convert $r$ to $\\hat{\\beta_x}$.\n\n$\\hat{r} = \\frac{covariance_{xy}}{s_x * s_y}$\n\n$\\hat{\\beta_x} = \\frac{covariance_{xy}}{s_x*s_x}$\n\n$covariance_{xy} = \\hat{r} * s_x * s_y$\n\n$\\hat{\\beta_x} = \\frac{\\hat{r} * s_x * s_y}{s_x * s_x} = r * \\frac{s_y}{s_x}$\n\n--\\> Regression slope = correlation multiplied by ratio of SDs (if SDs are equal, $r$ = $\\hat{\\beta}$ )\n\n::: notes\nEstimation of GLM:\n\nlinear algebra (R will do that for us!) --\\> Appendix book\n:::\n\n## Standard Errors for Regression Models\n\nWe usually want to make inferences about the regression parameter estimates. For this we need an estimate of their variability.\n\nWe first need an estimate of how much variability is *not* explained by the model: the **residual variance** (or **error variance**):\n\nCompute *residuals*:\n\n$$ residual = y - \\hat{y} = y - (x*\\hat{\\beta_x} + \\hat{\\beta_0}) $$\n\nCompute *Sum of Squared Errors* (remember?):\n\n$$ SS_{error} = \\sum_{i=1}^n{(y_i - \\hat{y_i})^2} = \\sum_{i=1}^n{residuals^2} $$\n\nCompute *Mean Squared Error*:\n\n$$ MS_{error} = \\frac{SS_{error}}{df} = \\frac{\\sum_{i=1}^n{(y_i - \\hat{y_i})^2} }{N - p} $$\n\nwhere the $df$ are the number of observations $N$ - the number of estimated parameter $p$ (in this case 2: $\\hat{\\beta_0}$ and $\\hat{\\beta_x}$).\n\nFinally, we can calculate the *standard error* for the *full* model:\n\n$$ SE_{model} = \\sqrt{MS_{error}} $$\n\nWe can also calculate the SE for specific regression parameter estimates by rescaling the $SE_{model}$:\n\n$$ SE_{\\hat{\\beta_x}} = \\frac{SE_{model}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}}} $$\n\n::: notes\n::: notes\nrescaling SE: by square root of the SS of the X variable\n:::\n:::\n\n## Statistical Tests for Regression Parameters\n\nWith the parameter estimates and their standard errors, we can compute $t$-statistics, which represent the likelihood of the observed estimate vs. the expected value under $H_0$ (usually 0, no effect).\n\n$$ \\begin{array}{c} t_{N - p} = \\frac{\\hat{\\beta} - \\beta_{expected}}{SE_{\\hat{\\beta}}}\\\\ t_{N - p} = \\frac{\\hat{\\beta} - 0}{SE_{\\hat{\\beta}}}\\\\ t_{N - p} = \\frac{\\hat{\\beta} }{SE_{\\hat{\\beta}}} \\end{array} $$\n\nUsually, we would just let R do the calculations:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = grade ~ studyTime, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.656  -2.719   0.125   4.703   7.469 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    76.16       5.16   14.76  6.1e-06 ***\nstudyTime       4.31       2.14    2.01    0.091 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.4 on 6 degrees of freedom\nMultiple R-squared:  0.403,\tAdjusted R-squared:  0.304 \nF-statistic: 4.05 on 1 and 6 DF,  p-value: 0.0907\n```\n:::\n:::\n\n\nThe intercept is significantly different from zero (which is usually not very relevant) and the effect of `studyTime` is not significant. So for every hour that we study more, the effect on the grade is rather small (4...) but possibly not present.\n\n::: notes\n$t$ ratio of $\\beta$ to its $SE$!\n\nintercept: expected grade without studying at all\n:::\n\n## Quantifying Goodness of Fit of the Model\n\nOften, it is useful to check how good the model we estimated fits the data.\n\n. . .\n\nWe can do that easily by asking *how much of the variability in the data is accounted for by the model?*\n\n. . .\n\nIf we only have one IV ($x$), then we can simply square the correlation coefficient:\n\n$$ R^2 = r^2 $$\n\nIn study time example, $R^2$ = 0.4 --\\> we accounted for 40% of the overall variance in grades!\n\n. . .\n\nMore generally, we can calculate $R^2$ with the Sum of Squared Variances:\n\n$$ R^2 = \\frac{SS_{model}}{SS_{total}} = 1-\\frac{SS_{error}}{SS_{total}} $$\n\n::: notes\n$R^2$ is the name of the GoF stat!\n\nA small R² tells us that even though a model might be significant, it may only explain a small amount of information in the DV\n:::\n\n## Fitting More Complex Models\n\nOften we want to know the effects of *multiple variables* (IVs) on some outcome.\n\nExample:\\\nSome students have taken a very similar class before, so there might not only be the effect of `studyTime` on `grades`, but also of having taken a `priorClass`.\n\n. . .\n\n::: columns\n::: column\nWe can built a model that takes both into account by simply adding the \"weight\" and the IV (`priorClass`) to the model:\n\n$\\hat{y} = \\hat{\\beta_1}*studyTime + \\hat{\\beta_2}*priorClass + \\hat{\\beta_0}$\n\n::: incremental\n-   To model `priorClass`, i.e. whether each individual has taken a previous class or not, we use **dummy coding** (0=no, 1=yes).\n-   This means, for those who have *not* taken a class, the whole part of the equation ($\\hat{\\beta_2} * priorClass$) will be zero - we will add it for the others.\n-   $\\hat{\\beta_2}$ is thus the difference in means between the two groups!\n-   $\\hat{\\beta_1}$ is the regression slope of `studyTime` across data points/regardless of whether someone has taken a class before.\n:::\n:::\n\n::: column\nIf we plot the data, we can see that both IVs seem to have an effect on grades:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-10-1.png){width=480}\n:::\n:::\n\n:::\n:::\n\nHow can we tell from the plot that both IVs might have an effect?\n\n## Interactions Between Variables\n\nWe previously assumed that the effect of `studyTime` on `grade` was the same for both groups - but sometimes we expect that this regression slope differs per group!\n\n. . .\n\nThis is what we call an **interaction**: The effect of one variable depends on the value of another variable.\n\n. . .\n\nExample: What is the effect of caffeine on public speaking?\\\nThere doesn't seem to be an effect:\n\n::: columns\n::: column\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform linear regression with caffeine as independent variable\nlmResultCaffeine <- lm(speaking ~ caffeine, data = df)\nsummary(lmResultCaffeine)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = speaking ~ caffeine, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-33.10 -16.02   5.01  16.45  26.98 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -7.413      9.165   -0.81     0.43\ncaffeine       0.168      0.151    1.11     0.28\n\nResidual standard error: 19 on 18 degrees of freedom\nMultiple R-squared:  0.0642,\tAdjusted R-squared:  0.0122 \nF-statistic: 1.23 on 1 and 18 DF,  p-value: 0.281\n```\n:::\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-13-1.png){width=384}\n:::\n:::\n\n:::\n:::\n\n## Interactions 2\n\nWhat if we find research suggesting that *anxious* people react differently to caffeine than non-anxious people?\n\nLet's include `anxiety` in the model:\n\n::: columns\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute linear regression adding anxiety to model\nlmResultCafAnx <- lm(speaking ~ caffeine + anxiety, data = df)\nsummary(lmResultCafAnx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = speaking ~ caffeine + anxiety, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-32.97  -9.74   1.35  10.53  25.36 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)\n(Intercept)        -12.581      9.197   -1.37     0.19\ncaffeine             0.131      0.145    0.91     0.38\nanxietynotAnxious   14.233      8.232    1.73     0.10\n\nResidual standard error: 18 on 17 degrees of freedom\nMultiple R-squared:  0.204,\tAdjusted R-squared:  0.11 \nF-statistic: 2.18 on 2 and 17 DF,  p-value: 0.144\n```\n:::\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-15-1.png){width=384}\n:::\n:::\n\n:::\n:::\n\n. . .\n\nIt looks like the effect of caffeine is indeed different for the two anxiety groups: Increasing for non-anxious people and decreasing for anxious ones.\n\nHowever, the model is not significant!\n\n. . .\n\nThis is due to the fact that we only look at **additive effects** (main effects) with this model. *Overall*, neither caffeine nor anxiety predicts grades.\n\nIn other words: The model tries to fit the same slope for both groups, which is a flat line.\n\n::: notes\nexplain additive effects: flat line for average caffeine effect, no difference for means of anxiety groups\n:::\n\n## Interactions 3\n\nTo allow for different slopes for each group (i.e. for the effect of caffeine to vary between the anxiety groups), we have to model the *interaction* as well.\n\nThe interaction is simply the product of the two variables:\n\n::: columns\n::: column\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute linear regression including caffeine X anxiety interaction\nlmResultInteraction <- lm(\n  speaking ~ caffeine + anxiety + caffeine:anxiety,\n  # speaking ~ caffeine * anxiety,  # same!\n  data = df\n)\nsummary(lmResultInteraction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = speaking ~ caffeine + anxiety + caffeine:anxiety, \n    data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.385  -7.103  -0.444   6.171  13.458 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 17.4308     5.4301    3.21  0.00546 ** \ncaffeine                    -0.4742     0.0966   -4.91  0.00016 ***\nanxietynotAnxious          -43.4487     7.7914   -5.58  4.2e-05 ***\ncaffeine:anxietynotAnxious   1.0839     0.1293    8.38  3.0e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.1 on 16 degrees of freedom\nMultiple R-squared:  0.852,\tAdjusted R-squared:  0.825 \nF-statistic: 30.8 on 3 and 16 DF,  p-value: 7.01e-07\n```\n:::\n:::\n\n:::\n\n::: column\n\n::: {.cell}\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-17-1.png){width=384}\n:::\n:::\n\n:::\n:::\n\n. . .\n\nWe now see that there are significant *main effects* for both `caffeine` and `anxiety`, as well as the significant *interaction* between both variables. (We have to be careful of interpreting the main effects when an interaction is also significant!)\n\n. . .\n\nThe interpretation of the coefficients when interactions are included is not as straight forward!\n\n. . .\n\nIf you want to report the \"typical\" ANOVA table with main effects and the general interaction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lmResultInteraction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: speaking\n                 Df Sum Sq Mean Sq F value Pr(>F)    \ncaffeine          1    455     455    6.96 0.0179 *  \nanxiety           1    992     992   15.17 0.0013 ** \ncaffeine:anxiety  1   4593    4593   70.27  3e-07 ***\nResiduals        16   1046      65                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n::: notes\ninterpretation coefficients:\n\nintercept: intercept of anxious group!\n\nintercept not anxious: difference intercept anxiousnotanxious\n\nslope anxious: only for the anxious group!\n\nslope not anxious: diff in slopes\n\nno main effects!!!\n:::\n\n## Model Comparison\n\nSometimes, we want to compare two (*nested*!) models to see which one fits the data better.\n\nWe can do so by using the `anova()`\\* function in R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lmResultCafAnx, lmResultInteraction) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: speaking ~ caffeine + anxiety\nModel 2: speaking ~ caffeine + anxiety + caffeine:anxiety\n  Res.Df  RSS Df Sum of Sq    F Pr(>F)    \n1     17 5639                             \n2     16 1046  1      4593 70.3  3e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThis shows that Model 2, incl. the interaction, is to be preferred.\n\n. . .\n\n*Note*: We can only use this method with nested models, which means that the simpler (*reduced*) model only contains variables also included in the more complex (*full*) model.\n\n::: aside\n\\*Yes, it is *kind of* an ANOVA as well, in that (a ratio of) squared errors is compared to an $F$-distribution...\n:::\n\n::: notes\nWald compares the ratio of squared errors to an F-distribution (sound familiar from ANOVA?), while likelihood ratio compares the ratio of likelihoods to a χ2 distribution\n:::\n\n## Criticizing Our Model and Checking Assumptions\n\n\"Garbage in, garbage out\" - we have to make sure our model is properly specified!\n\n. . .\n\n*Properly specified* = having included the appropriate IVs.\n\n. . .\n\nThe model also needs to satisfy the **assumptions** of the statistical method (= GLM).\n\nOne important assumption of the GLM is that *the residuals are normally distributed* (NOT necessarily the data!).\n\nThis assumption can be violated by a not properly specified model or because the data are inappropriate for the statistical model.\n\n. . .\n\nWe can use a **Q-Q plot**, which represents the quantiles of two distributions/variables (e.g. the data and a normal distribution of the same data) against each other.\n\nIf the data points diverge substantially from the line (especially in the extremes), we can conclude that the residuals are not normally distributed.\n\n## Model Diagnostics 2\n\nTo check the assumptions, we can easily run a function for model diagnostics (incl. Q-Q plots) in R. The function, `check_model()`, is included in the `performance` package by the [*easystats*](https://easystats.github.io/easystats/index.html) team (who make great packages for everything related to statistical modeling!)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"easystats) \nlibrary(performance)  \n\ncheck_model(lmResultInteraction)\n```\n\n::: {.cell-output-display}\n![](Stats1_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\nWe're not going into detail about all these diagnostics (and hard to see!), but it is always a good idea to run diagnostics/check assumptions for your models!\n\n## What Does \"Predict\" Really Mean?\n\nWe neither mean \"predicting before seeing the data/in the future\" nor mean to imply *causality*!\n\n. . .\n\nIt simply refers to fitting a model to the data: We estimate (or predict) values for the DV ($\\hat{y}$) and the IVs are often referred to as *predictors*.\n\nRelated to: predicting future values\n\n## ANOVA\n\nAs you saw earlier, the \"normal\" ANOVA is a special case of the linear model.\n\nThe difference is that the linear model is a bit more flexible in terms of including different IVs/predictors (continuous & categorical), whereas the ANOVA can only use categorical IVs (unless you run an ANCOVA).\n\n. . .\n\nAn ANOVA is basically an F-Test that we have used previously (e.g. anova(modelfit)).\n\nThe F statistic is calculated by differentiating Sum of Squares Within (SSW) and Sum of Squares Between (SSB) categories/factor levels:\n\n$$\nSSW = \\sum_{i=1}^n{(y_{ij} - \\bar{y_j})^2}\n$$\n\nSSW is the (squared and summed) difference between each data point and it's category's mean.\n\n$$\nSSB = \\sum_{i=1}^n{(\\bar{y_j} - \\bar{y})^2}\n$$\n\nSSB is the (squared and summed) difference between each category's mean and the overall mean.\n\n$$\nF = \\frac{SSB/(M*(N-1))}{SSW/(M-1)}\n$$\n\nM = number of categories/groups/factor levels; N = number of participants/observations\n\n# Repeated Measures ANOVA & Linear Mixed Model\n\nNext time?\n\nProblem: Dependent measures!\n\n# Thanks!\n",
    "supporting": [
      "Stats1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}